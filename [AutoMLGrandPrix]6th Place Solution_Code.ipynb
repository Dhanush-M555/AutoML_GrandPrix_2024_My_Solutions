{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_and_import(package):\n",
    "    try:\n",
    "        __import__(package)\n",
    "    except ImportError:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "        __import__(package)\n",
    "\n",
    "packages = [\n",
    "    ('sklearn', '*'), \n",
    "    ('tqdm', 'tqdm'), \n",
    "    ('re', 're'), \n",
    "    ('numpy', 'np'), \n",
    "    ('pandas', 'pd'), \n",
    "    ('matplotlib.pyplot', 'plt'), \n",
    "    ('seaborn', 'sns'), \n",
    "    ('os', 'os'), \n",
    "    ('warnings', 'warnings'), \n",
    "    ('xgboost', 'xgboost'), \n",
    "    ('catboost', 'catboost'), \n",
    "    ('lightgbm', 'lightgbm'), \n",
    "    ('tabulate', 'tabulate'), \n",
    "    ('statsmodels', 'ARIMA'), \n",
    "    ('colorama', 'Fore, Style, init'), \n",
    "    ('category_encoders', '*'), \n",
    "    ('mlxtend', '*'), \n",
    "    ('optuna', 'optuna'), \n",
    "]\n",
    "\n",
    "# Install and import packages\n",
    "for package, import_as in packages:\n",
    "    install_and_import(package)\n",
    "\n",
    "# Specific imports\n",
    "from sklearn.pipeline import *\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import warnings\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.metrics import *\n",
    "from sklearn.tree import *\n",
    "from sklearn.ensemble import *\n",
    "from xgboost import *\n",
    "from catboost import *\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.isotonic import *\n",
    "from sklearn.compose import *\n",
    "from sklearn.neural_network import *\n",
    "from lightgbm import *\n",
    "from tabulate import tabulate\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from colorama import Fore, Style, init\n",
    "from category_encoders import *\n",
    "from mlxtend.evaluate import *\n",
    "from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\n",
    "from sklearn.utils.validation import _deprecate_positional_args\n",
    "from sklearn.base import *\n",
    "from sklearn.inspection import *\n",
    "import optuna\n",
    "from optuna.visualization import *\n",
    "from optuna.pruners import *\n",
    "import logging\n",
    "import sys\n",
    "from sklearn.decomposition import *\n",
    "\n",
    "# Add stream handler of stdout to show the messages to see Optuna works expectedly.\n",
    "optuna.logging.get_logger(\"optuna\").addHandler(logging.StreamHandler(sys.stdout))\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "# warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "# warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##These Features quite helped LGBM model, but I did not use it for final blend!\n",
    "class FeatureEngineering(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):        \n",
    "        x = X.copy()\n",
    "        \n",
    "        x['sum'] = x['capdiameter'] + x['stemheight'] + x['stemwidth']\n",
    "        x['capheightproduct'] = x['capdiameter'] * x['stemheight']\n",
    "        x['capwidthproduct'] = x['capdiameter'] * x['stemwidth']\n",
    "        x['heightwidthproduct'] = x['stemheight'] * x['stemwidth']\n",
    "        x['product'] = x['capdiameter'] * x['stemheight'] * x['stemwidth']\n",
    "\n",
    "        # Adding a small value to avoid division by zero\n",
    "        # x['captostemheight'] = x['capdiameter'] / (x['stemheight'] + 1e-6)\n",
    "        # x['captostemwidth'] = x['capdiameter'] / (x['stemwidth'] + 1e-6)\n",
    "        x['stemheighttowidth'] = x['stemheight'] / (x['stemwidth'].replace(0, np.nan) + 1e-6)\n",
    "        x['capheightdifference'] = x['capdiameter'] - x['stemheight']\n",
    "        x['capwidthdifference'] = x['capdiameter'] - x['stemwidth']\n",
    "        x['stemheightwidthdifference'] = x['stemheight'] - x['stemwidth']\n",
    "\n",
    "        x.replace([np.inf, -np.inf],0, inplace=True)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "train = pl.read_csv(\"/kaggle/input/playground-series-s4e8/train.csv\").drop(\"id\").to_pandas()\n",
    "print(\"Train Dataset =\", train.shape)\n",
    "display(train)\n",
    "\n",
    "\n",
    "test = pl.read_csv(\"/kaggle/input/playground-series-s4e8/test.csv\").drop(\"id\").to_pandas()\n",
    "print(\"Test Dataset =\", test.shape)\n",
    "display(test)\n",
    "\n",
    "\n",
    "submission = pd.read_csv(\"/kaggle/input/playground-series-s4e8/sample_submission.csv\")\n",
    "\n",
    "\n",
    "# origin = pl.read_csv(\"/kaggle/input/playground-series-s4e8/train.csv\").to_pandas()\n",
    "\n",
    "origin=train.copy()\n",
    "print(\"Original Dataset =\", origin.shape)\n",
    "display(origin)\n",
    "\n",
    "\n",
    "target = \"class\"\n",
    "\n",
    "train.columns    = train.columns.str.replace(\"-\",\"\")\n",
    "test.columns     = test.columns.str.replace(\"-\",\"\")\n",
    "origin.columns = origin.columns.str.replace(\"-\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credits to Ambros notebooks:\n",
    "float_features = ['capdiameter', 'stemheight', 'stemwidth']\n",
    "initial_features = test.columns\n",
    "cat_features = [f for f in initial_features if f not in float_features]\n",
    "\n",
    "for feature in initial_features:\n",
    "    if feature in cat_features:\n",
    "        # Ensure categorical features are processed\n",
    "        categories = sorted(list(set(df[feature].dropna()) | set(test[feature].dropna())))\n",
    "        dtype = pd.CategoricalDtype(categories=categories, ordered=False)\n",
    "        print(f\"{feature:30} {len(dtype.categories)}\")\n",
    "        \n",
    "        # Apply categorical dtype\n",
    "        df[feature] = df[feature].astype(dtype)\n",
    "        test[feature] = test[feature].astype(dtype)\n",
    "    else:\n",
    "        # Handle non-numeric values in float features\n",
    "        df[feature] = pd.to_numeric(df[feature], errors='coerce')\n",
    "        test[feature] = pd.to_numeric(test[feature], errors='coerce')\n",
    "        \n",
    "        # Convert to float32\n",
    "        dtype = np.float32\n",
    "        df[feature] = df[feature].astype(dtype)\n",
    "        test[feature] = test[feature].astype(dtype)\n",
    "\n",
    "display(df)\n",
    "display(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[target]=df[target].map({'e':0,'p':1}).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_feats=['capshape',\n",
    "    'capsurface',\n",
    "    'capcolor',\n",
    "    'doesbruiseorbleed',\n",
    "    'gillattachment',\n",
    "    'gillspacing',\n",
    "    'gillcolor',\n",
    "    'stemroot',\n",
    "    'stemsurface',\n",
    "    'stemcolor',\n",
    "    'veiltype',\n",
    "    'veilcolor',\n",
    "    'hasring',\n",
    "    'ringtype',\n",
    "    'sporeprintcolor',\n",
    "    'habitat',\n",
    "    'season']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df.drop(target,axis=1)\n",
    "y=df[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb=XGBClassifier(**{\n",
    "    'enable_categorical': True,\n",
    "    'device': 'cuda',\n",
    "    'n_estimators': 417,         \n",
    "    'learning_rate': 0.06743,           \n",
    "    'max_depth': 18,                \n",
    "    'colsample_bytree': 0.567,         \n",
    "    'min_child_weight': 4,         \n",
    "    'reg_lambda':73,          \n",
    "    'subsample': 1,              \n",
    "    'num_parallel_tree': 5,\n",
    "}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CVBlender(model, df, df_test, target, cv=\"skf\", seed=seed):\n",
    "    X = df.drop(target, axis=1)\n",
    "    y = df[target]\n",
    "    bags_mean = []\n",
    "    foldwise_accuracy = []\n",
    "    \n",
    "    if cv == \"kf\":\n",
    "        cv_method = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    elif cv == \"skf\":\n",
    "        cv_method = StratifiedKFold(n_splits=25, shuffle=True, random_state=seed)\n",
    "    elif cv == \"rskf\":\n",
    "        cv_method = RepeatedStratifiedKFold(n_splits=5, n_repeats=2, random_state=seed)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid cross-validation method. Please choose 'kf', 'skf', or 'rskf'.\")\n",
    "    \n",
    "    counter = 0\n",
    "    for train_idx, valid_idx in cv_method.split(X, y):\n",
    "        X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
    "        X_val, y_val = X.iloc[valid_idx], y.iloc[valid_idx]\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_val)\n",
    "        acc = matthews_corrcoef(y_val, y_pred)\n",
    "        print(f\"Matthews Correlation Coefficient for fold {counter + 1}: {acc}\")\n",
    "        foldwise_accuracy.append(acc)\n",
    "        \n",
    "        y_pred_test = model.predict_proba(df_test)[:, 1]\n",
    "        bags_mean.append(y_pred_test)\n",
    "        \n",
    "        print(f\"Average Matthews Correlation Coefficient Across Fold {counter + 1}: {acc}\")\n",
    "        counter += 1\n",
    "\n",
    "    final_accuracy = np.mean(foldwise_accuracy)\n",
    "    print(f\"Final Total Average Matthews Correlation Coefficient: {final_accuracy}\")\n",
    "    \n",
    "    total_mean_predictions = np.mean(bags_mean, axis=0)\n",
    "    return total_mean_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds=CVBlender(xgb1,df,test,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_sub=submission.copy()\n",
    "xgb_sub[target]=preds\n",
    "xgb_sub.to_csv('XGB_tunned_25Folds.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoGluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install autogluon.tabular\n",
    "\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TabularDataset(df)\n",
    "test_data=TabularDataset(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_tune_kwargs = {  \n",
    "    'num_trials': 54,\n",
    "    'scheduler' : 'local',\n",
    "    'searcher'  : 'auto',\n",
    "}\n",
    "excluded_model_types=['KNN','XTModel']\n",
    "predictor = TabularPredictor(label = target,\n",
    "                                eval_metric='mcc',\n",
    "                                problem_type = 'binary',\n",
    "                                \n",
    "                            )\n",
    "predictor.fit(train_data,\n",
    "            ag_args_fit={'num_gpus': 1, 'num_cpus': 4},\n",
    "            time_limit = 8*60*60,\n",
    "            hyperparameter_tune_kwargs=hyperparameter_tune_kwargs,\n",
    "            presets = 'best_quality',\n",
    "            save_space = True,\n",
    "            keep_only_best = True,\n",
    "            excluded_model_types=excluded_model_types\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds_proba=predictor.predict_proba(test_data)\n",
    "sub=submission.copy()\n",
    "sub=sub.drop(target,axis=1)\n",
    "sub['autogluon']=test_preds_proba.iloc[:,1]\n",
    "sub.to_csv('autogluon_probas.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_submission=submission.copy()\n",
    "\n",
    "final_submission[target]=0.3*xgb_sub[target]+0.7*sub[target]\n",
    "final_submission[target]=(final_submission[target]>0.5)\n",
    "final_submission[target]=final_submission[target].map({False:'e',True:'p'})\n",
    "final_submission.to_csv('xgb_folds_tunned1.csv',index=False)\n",
    "display(final_submission)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This notebook combines two separate notebooks into one for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
