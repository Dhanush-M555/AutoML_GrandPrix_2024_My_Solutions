{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_and_import(package):\n",
    "    try:\n",
    "        __import__(package)\n",
    "    except ImportError:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "        __import__(package)\n",
    "\n",
    "packages = [\n",
    "    ('sklearn', '*'), \n",
    "    ('tqdm', 'tqdm'), \n",
    "    ('re', 're'), \n",
    "    ('numpy', 'np'), \n",
    "    ('pandas', 'pd'), \n",
    "    ('matplotlib.pyplot', 'plt'), \n",
    "    ('seaborn', 'sns'), \n",
    "    ('os', 'os'), \n",
    "    ('warnings', 'warnings'), \n",
    "    ('xgboost', 'xgboost'), \n",
    "    ('catboost', 'catboost'), \n",
    "    ('lightgbm', 'lightgbm'), \n",
    "    ('tabulate', 'tabulate'), \n",
    "    ('statsmodels', 'ARIMA'), \n",
    "    ('colorama', 'Fore, Style, init'), \n",
    "    ('category_encoders', '*'), \n",
    "    ('mlxtend', '*'), \n",
    "    ('optuna', 'optuna'), \n",
    "]\n",
    "\n",
    "# Install and import packages\n",
    "for package, import_as in packages:\n",
    "    install_and_import(package)\n",
    "\n",
    "# Specific imports\n",
    "from sklearn.pipeline import *\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import warnings\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.metrics import *\n",
    "from sklearn.tree import *\n",
    "from sklearn.ensemble import *\n",
    "from xgboost import *\n",
    "from catboost import *\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.isotonic import *\n",
    "from sklearn.compose import *\n",
    "from sklearn.neural_network import *\n",
    "from lightgbm import *\n",
    "from tabulate import tabulate\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from colorama import Fore, Style, init\n",
    "from category_encoders import *\n",
    "from mlxtend.evaluate import *\n",
    "from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\n",
    "from sklearn.utils.validation import _deprecate_positional_args\n",
    "from sklearn.base import *\n",
    "from sklearn.inspection import *\n",
    "import optuna\n",
    "from optuna.visualization import *\n",
    "from optuna.pruners import *\n",
    "import logging\n",
    "import sys\n",
    "from sklearn.decomposition import *\n",
    "\n",
    "# Add stream handler of stdout to show the messages to see Optuna works expectedly.\n",
    "optuna.logging.get_logger(\"optuna\").addHandler(logging.StreamHandler(sys.stdout))\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "# warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "# warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UseOriginalDataset=False\n",
    "UseAdditionalData=False\n",
    "UseMyEncoding=False\n",
    "UseFE=True\n",
    "\n",
    "n_splits=2\n",
    "n_repeats=3\n",
    "seed=0\n",
    "copies=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "def create_folds(X_train, y_train, origin, n_splits=2,n_repeats=n_repeats,copies=1, random_state=None):\n",
    "    folds = []\n",
    "    kf =RepeatedKFold(n_splits=n_splits,n_repeats=n_repeats,random_state=0)\n",
    "    target_bins = np.floor(y_train).astype(int)\n",
    "    for train_index, valid_index in kf.split(X_train,target_bins):\n",
    "        if UseOriginalDataset:\n",
    "            train_index_with_origin = np.concatenate((train_index,np.arange(copies*len(origin)) + len(X_train)))\n",
    "            folds.append((train_index_with_origin, valid_index))\n",
    "        else:\n",
    "            folds.append((train_index, valid_index))\n",
    "    return folds\n",
    "\n",
    "\n",
    "def get_combined_data(df,original,copies):\n",
    "    for _ in range(copies):\n",
    "        df = pd.concat([df,original],ignore_index=True)\n",
    "    return \n",
    "\n",
    "\n",
    "def simple_cv(model, X, y, folds, silent=True):\n",
    "    r2_scores = []\n",
    "    for train_index, valid_index in folds:\n",
    "        X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n",
    "        y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
    "        model.fit(X_train,y_train)\n",
    "        y_pred = model.predict(X_valid)\n",
    "        \n",
    "#             y_pred = np.array(y_pred).reshape(-1, 1)\n",
    "#             y_pred = scaler.inverse_transform(y_pred)\n",
    "\n",
    "        r2 =  mean_squared_error(y_valid, y_pred,squared=False)#Change metric\n",
    "        r2_scores.append(r2)\n",
    "    if not silent:\n",
    "        print(\"Cross-Validation Scores:\", r2_scores)\n",
    "        print(\"Maximum CV score:\", np.min(r2_scores))\n",
    "        print(\"Mean CV Score:\", np.mean(r2_scores))\n",
    "    return np.mean(r2_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "\n",
    "# Read train dataset and convert to pandas\n",
    "train = pl.read_csv(\"/kaggle/input/playground-series-s4e9/train.csv\").drop(\"id\").to_pandas()\n",
    "print(\"Train Dataset =\", train.shape)\n",
    "display(train)\n",
    "\n",
    "# Read test dataset and convert to pandas\n",
    "test = pl.read_csv(\"/kaggle/input/playground-series-s4e9/test.csv\").drop(\"id\").to_pandas()\n",
    "print(\"Test Dataset =\", test.shape)\n",
    "display(test)\n",
    "\n",
    "# Read sample submission dataset using pandas\n",
    "submission = pd.read_csv(\"/kaggle/input/playground-series-s4e9/sample_submission.csv\")\n",
    "\n",
    "# Read original dataset and convert to pandas\n",
    "origin = pl.read_csv(\"/kaggle/input/used-car-price-prediction-dataset/used_cars.csv\").to_pandas()\n",
    "print(\"Original Dataset =\", origin.shape)\n",
    "display(origin)\n",
    "\n",
    "\n",
    "add_data=pl.read_csv(\"/kaggle/input/kagglex-official-dataset/train.csv\").drop(\"id\").to_pandas()\n",
    "print(\"Additional Original Dataset =\", add_data.shape)\n",
    "display(add_data)\n",
    "\n",
    "\n",
    "\n",
    "# Target variable\n",
    "target = \"price\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if UseOriginalDataset:\n",
    "    df=get_combined_data(train,origin,copies)\n",
    "else:\n",
    "    df=train.copy()\n",
    "    \n",
    "    \n",
    "if UseAdditionalData:\n",
    "    df=get_combined_data(df,add_data,copies)\n",
    "\n",
    "\n",
    "num_duplicates = df.duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {num_duplicates}\")\n",
    "df = df.drop_duplicates(keep='last')\n",
    "num_duplicates = df.duplicated().sum()\n",
    "print(f\"Number of duplicate rows After Removal: {num_duplicates}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Missing Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['fuel_type'] = df['fuel_type'].fillna('none')\n",
    "test['fuel_type'] = test['fuel_type'].fillna('none')\n",
    "\n",
    "df['accident'] = df['accident'].fillna('empty')\n",
    "test['accident'] = test['accident'].fillna('empty')\n",
    "\n",
    "df['clean_title'] = df['clean_title'].fillna('empty')\n",
    "test['clean_title'] = test['clean_title'].fillna('empty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if UseMyEncoding:\n",
    "    unknown_test_model_col=list(set(test['model'].unique())-set(df['model'].unique()))\n",
    "    print(unknown_test_model_col)\n",
    "    # unknown_origin_model_col=list(set(origin['model'].unique())-set(df['model'].unique()))\n",
    "    # print(unknown_origin_model_col)\n",
    "    thresholds=[1,5]\n",
    "    value_counts = df['model'].value_counts()\n",
    "    rare_categories_dict = {f'rare{i+1}': [] for i in range(len(thresholds))}\n",
    "    def assign_rare_category(color, value_counts, thresholds):\n",
    "        if color in unknown_test_model_col:\n",
    "            rare_categories_dict['rare1'].append(color)\n",
    "            return 'rare1'\n",
    "        for i, threshold in enumerate(thresholds):\n",
    "            if value_counts[color] <= threshold:\n",
    "                if i == 0 or value_counts[color] > thresholds[i - 1]:\n",
    "                    rare_categories_dict[f'rare{i+1}'].append(color)\n",
    "                    return f'rare{i+1}'\n",
    "        return color\n",
    "    df['model'] =df['model'].apply(assign_rare_category, value_counts=value_counts, thresholds=thresholds)\n",
    "    # origin['model'] = origin['model'].apply(lambda x: assign_rare_category(x, value_counts=value_counts, thresholds=thresholds))\n",
    "    test['model'] = test['model'].apply(lambda x: assign_rare_category(x, value_counts=value_counts, thresholds=thresholds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['engine'] = test['engine'].replace(\n",
    "    '177.0HP 1.6L 4 Cylinder Engine Plug-In Electric/Gas',\n",
    "    '139.0HP 1.6L 4 Cylinder Engine Plug-In Electric/Gas'\n",
    ")\n",
    "\n",
    "# Extract horsepower\n",
    "def engine_feat_extract(data):\n",
    "    dff=data.copy()\n",
    "    dff['horsepower'] = dff['engine'].str.extract(r'(\\d+\\.\\d+)HP')\n",
    "    \n",
    "    dff['engine'] = dff['engine'].apply(lambda x: x.replace(' Litre', 'L'))\n",
    "    dff['engine'] = dff['engine'].apply(lambda x: x.replace(' Liter', 'L'))\n",
    "    dff['engine'] = dff['engine'].apply(lambda x: x.replace(' L', 'L'))\n",
    "    dff['engine'] = dff['engine'].apply(lambda x: x.replace('V-', 'V'))\n",
    "    \n",
    "    dff['engine_capacity'] = dff['engine'].str.extract(r'(\\d+\\.\\d+)L')\n",
    "\n",
    "    dff['cylinder_type'] = dff['engine'].str.extract(r'( \\d+ | V\\d+ | I\\d+ | W\\d+ |I\\d+ |V\\d+ |V\\d+| H\\d+ |I\\d+)')\n",
    "    dff['cylinder_type'] = dff['cylinder_type'].str.strip()\n",
    "    \n",
    "    dff['is_electric'] = dff['engine'].str.contains('Electric').astype(int)\n",
    "    dff['DOHC'] = dff['engine'].str.contains('DOHC').astype(int)\n",
    "    dff['GDI'] = dff['engine'].str.contains('GDI').astype(int)\n",
    "    dff['MPFI'] = dff['engine'].str.contains('MPFI').astype(int)\n",
    "    \n",
    "#     dff['Gasoline'] = dff['engine'].str.contains('Gasoline').astype(int)\n",
    "#     dff['Fuel'] = dff['engine'].str.contains('Fuel').astype(int)\n",
    "    \n",
    "    dff['voltage'] = dff['engine'].str.extract(r'(\\d+V)')\n",
    "    dff['voltage'] = dff['voltage'].str.replace('V', '')\n",
    "    \n",
    "#     df['engine'] = df['engine'].str.replace(r' \\d+ | V\\d+ | I\\d+ | W\\d+ |I\\d+ |V\\d+ |V\\d+| H\\d+ |I\\d', '', regex=True)\n",
    "#     df['engine'] = df['engine'].str.replace(r'\\d+\\.\\d+L', '', regex=True)\n",
    "#     df['engine'] = df['engine'].str.replace(r'\\d+\\.\\d+HP', '', regex=True)\n",
    "#     df['engine'] = df['engine'].str.replace(r'\\d+V', '', regex=True)\n",
    "#     df['engine'] = df['engine'].str.replace(' Electric', '', regex=False)\n",
    "#     df['engine'] = df['engine'].str.replace('Electric', '', regex=False)\n",
    "#     df['engine'] = df['engine'].str.replace('DOHC', '', regex=False)\n",
    "#     df['engine'] = df['engine'].str.replace('GDI', '', regex=False)\n",
    "#     df['engine'] = df['engine'].str.replace('MPFI', '', regex=False)\n",
    "#     df['engine'] = df['engine'].str.strip()\n",
    "    return dff\n",
    "if UseFE:\n",
    "    df=engine_feat_extract(df)\n",
    "    test=engine_feat_extract(test)\n",
    "\n",
    "# df[['horsepower','engine_capacity','cylinder_type','is_electric']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handling missing values/Nan\n",
    "if UseFE:\n",
    "    engine_feats=['voltage','horsepower','engine_capacity','cylinder_type','is_electric','DOHC','GDI','MPFI']\n",
    "    columns_to_fill=['voltage','horsepower','engine_capacity','cylinder_type']\n",
    "    df[columns_to_fill] = df[columns_to_fill].fillna(-1)\n",
    "    test[columns_to_fill] = test[columns_to_fill].fillna(-1)\n",
    "\n",
    "    df[['voltage','horsepower','engine_capacity']]=df[['voltage','horsepower','engine_capacity']].astype(float)\n",
    "    test[['voltage','horsepower','engine_capacity']]=test[['voltage','horsepower','engine_capacity']].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transmission_feat_extract(data):\n",
    "    dff=data.copy()\n",
    "    dff['transmission'] = dff['transmission'].apply(lambda x: x.replace('Automatic', 'A/T'))\n",
    "    dff['transmission'] = dff['transmission'].apply(lambda x: x.replace('Manual', 'M/T'))\n",
    "    dff['transmission'] = dff['transmission'].apply(lambda x: x.replace('At', 'A/T'))\n",
    "    dff['transmission'] = dff['transmission'].apply(lambda x: x.replace('Mt', 'M/T'))\n",
    "    dff['transmission'] = dff['transmission'].apply(lambda x: x.replace('6 Speed', '6-Speed'))\n",
    "    dff['speed'] = dff['transmission'].str.extract(r'(\\d+-Speed|\\d)')\n",
    "    dff['speed']=dff['speed'].astype(str).apply(lambda x: x.replace('-Speed', ''))\n",
    "    dff['speed'] = dff['speed'].str.strip()\n",
    "#     dff['speed']=dff['speed'].astype(int)\n",
    "    \n",
    "    dff['AT'] = dff['transmission'].str.contains('A/T').astype(int)\n",
    "    dff['MT'] = dff['transmission'].str.contains('M/T').astype(int)\n",
    "    \n",
    "#     dff['transmission'] = dff['transmission'].str.replace(r'\\d+-Speed|\\d', '', regex=True)\n",
    "#     dff['transmission'] = dff['transmission'].str.replace('M/T', '', regex=False)\n",
    "#     dff['transmission'] = dff['transmission'].str.replace('A/T', '', regex=False)\n",
    "#     dff['transmission'] = dff['transmission'].str.split(',').str[0]\n",
    "#     dff['transmission'] = dff['transmission'].str.strip()\n",
    "    return dff\n",
    "\n",
    "if UseFE:\n",
    "    df=transmission_feat_extract(df)\n",
    "    test=transmission_feat_extract(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ext_col']=df['ext_col'].str.lower()\n",
    "test['ext_col']=test['ext_col'].str.lower()\n",
    "df['int_col']=df['int_col'].str.lower()\n",
    "test['int_col']=test['int_col'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if UseMyEncoding:\n",
    "    unknown_test_brand_col=list(set(test['brand'].unique())-set(df['brand'].unique()))\n",
    "    print(unknown_test_brand_col)\n",
    "    unknown_origin_brand_col=list(set(add_data['brand'].unique())-set(df['brand'].unique()))\n",
    "    print(unknown_origin_brand_col)\n",
    "\n",
    "    thresholds=[1,5]\n",
    "    value_counts = df['brand'].value_counts()\n",
    "    rare_categories_dict = {f'rare{i+1}': [] for i in range(len(thresholds))}\n",
    "    def assign_rare_category(color, value_counts, thresholds):\n",
    "        if color in unknown_test_brand_col or color in unknown_origin_brand_col:\n",
    "            rare_categories_dict['rare1'].append(color)\n",
    "            return 'rare1'\n",
    "        for i, threshold in enumerate(thresholds):\n",
    "            if value_counts[color] <= threshold:\n",
    "                if i == 0 or value_counts[color] > thresholds[i - 1]:\n",
    "                    rare_categories_dict[f'rare{i+1}'].append(color)\n",
    "                    return f'rare{i+1}'\n",
    "        return color\n",
    "    df['brand'] =df['brand'].apply(assign_rare_category, value_counts=value_counts, thresholds=thresholds)\n",
    "    origin['brand'] = origin['brand'].apply(lambda x: assign_rare_category(x, value_counts=value_counts, thresholds=thresholds))\n",
    "    test['brand'] = test['brand'].apply(lambda x: assign_rare_category(x, value_counts=value_counts, thresholds=thresholds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if UseMyEncoding:\n",
    "    unknown_test_int_col=list(set(test['int_col'].unique())-set(df['int_col'].unique()))\n",
    "    unknown_origin_int_col=list(set(add_data['int_col'].unique())-set(train['int_col'].unique()))\n",
    "    thresholds=[1,5,10]\n",
    "    value_counts = df['int_col'].value_counts()\n",
    "    rare_categories_dict = {f'rare{i+1}': [] for i in range(len(thresholds))}\n",
    "    def assign_rare_category(color, value_counts, thresholds):\n",
    "        if color in unknown_test_int_col or color in unknown_origin_int_col:\n",
    "            rare_categories_dict['rare1'].append(color)\n",
    "            return 'rare1'\n",
    "        for i, threshold in enumerate(thresholds):\n",
    "            if value_counts[color] <= threshold:\n",
    "                if i == 0 or value_counts[color] > thresholds[i - 1]:\n",
    "                    rare_categories_dict[f'rare{i+1}'].append(color)\n",
    "                    return f'rare{i+1}'\n",
    "        return color\n",
    "    df['int_col'] =df['int_col'].apply(assign_rare_category, value_counts=value_counts, thresholds=thresholds)\n",
    "    test['int_col'] = test['int_col'].apply(lambda x: assign_rare_category(x, value_counts=value_counts, thresholds=thresholds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if UseMyEncoding:\n",
    "    unknown_test_ext_col=list(set(test['ext_col'].unique())-set(df['ext_col'].unique()))\n",
    "    unknown_origin_ext_col=list(set(add_data['ext_col'].unique())-set(train['ext_col'].unique()))\n",
    "    thresholds=[1,5,10]\n",
    "    value_counts = df['ext_col'].value_counts()\n",
    "    rare_categories_dict = {f'rare{i+1}': [] for i in range(len(thresholds))}\n",
    "    def assign_rare_category(color, value_counts, thresholds):\n",
    "        if color in unknown_test_ext_col or color in unknown_origin_ext_col:\n",
    "            rare_categories_dict['rare1'].append(color)\n",
    "            return 'rare1'\n",
    "        for i, threshold in enumerate(thresholds):\n",
    "            if value_counts[color] <= threshold:\n",
    "                if i == 0 or value_counts[color] > thresholds[i - 1]:\n",
    "                    rare_categories_dict[f'rare{i+1}'].append(color)\n",
    "                    return f'rare{i+1}'\n",
    "        return color\n",
    "    df['ext_col'] =df['ext_col'].apply(assign_rare_category, value_counts=value_counts, thresholds=thresholds)\n",
    "    test['ext_col'] = test['ext_col'].apply(lambda x: assign_rare_category(x, value_counts=value_counts, thresholds=thresholds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "\n",
    "categorical_columns = test.select_dtypes(include=['object']).columns\n",
    "for col in categorical_columns:\n",
    "    df[col] = df[col].astype(str)\n",
    "    test[col] = test[col].astype(str)\n",
    "    \n",
    "categorical_columns = list(categorical_columns)\n",
    "cat_cols=categorical_columns\n",
    "print(categorical_columns)\n",
    "# categorical_columns.append('model_year')\n",
    "\n",
    "for col in categorical_columns:\n",
    "    print(col)\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    test[col] = le.transform(test[col])\n",
    "\n",
    "\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_feats=['brand', 'model','engine', 'int_col', 'ext_col','accident','cylinder_type','transmission','fuel_type']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "\n",
    "# data=df.copy()\n",
    "# data=data.reset_index().drop('index',axis=1)\n",
    "# X=data.drop(target,axis=1)\n",
    "# rng = np.random.default_rng(42)\n",
    "# X['NOISE1'] = rng.normal(size=len(X))\n",
    "# X['NOISE2'] = rng.normal(size=len(X))\n",
    "# X['NOISE3'] = rng.normal(size=len(X))\n",
    "\n",
    "# n_splits=2\n",
    "# n_repeats=3\n",
    "# kfold =RepeatedKFold(n_splits=n_splits,n_repeats=n_repeats,random_state=0)\n",
    "# clfs = {\n",
    "#     'lightgbm':hgb,\n",
    "# #     'hist_gradient_boosting':hgb,\n",
    "# #     'gradient_boosting':gb,\n",
    "# }\n",
    "\n",
    "# # for col in labels:\n",
    "# #     print(f\"\\n{'=' * 20} For TARGET: {col} {'=' * 20}\\n\")\n",
    "# y=data[target]\n",
    "# _, axes = plt.subplots(len(clfs),1,figsize=(20, len(clfs)*5))\n",
    "# for i,m in enumerate(clfs):\n",
    "#     scores = 0\n",
    "#     roc_auc_scores=[]\n",
    "#     with tqdm(total=n_splits*n_repeats) as pbar:\n",
    "#         for train_index, valid_index in kfold.split(X,y):\n",
    "#             X_t, X_v = X.iloc[train_index], X.iloc[valid_index]\n",
    "#             y_t, y_v = y[train_index], y[valid_index]\n",
    "#             model=clone(clfs[m])\n",
    "#             model.fit(X_t,y_t)\n",
    "#             y_pred=model.predict(X_v)\n",
    "#             roc_auc =  mean_squared_error(y_v, y_pred,squared=False)#Change the metric\n",
    "#             roc_auc_scores.append(roc_auc)\n",
    "#             scores += permutation_importance(clone(clfs[m]).fit(X_t,y_t),\n",
    "#                                              X_v,y_v,\n",
    "#                                              scoring='neg_root_mean_squared_error',#Change the metric\n",
    "#                                              n_jobs=-1,\n",
    "#                                              random_state=seed).importances_mean\n",
    "#             pbar.update()\n",
    "#         pbar.close()\n",
    "#     roc_auc_scores = np.array(roc_auc_scores)\n",
    "#     print(f\"Average Accuracy score of {m}==>{roc_auc_scores.mean()} ± {roc_auc_scores.std()}\")\n",
    "#     print(f\"Minimum Accuracy score of {m}==>{np.min(roc_auc_scores)}\")\n",
    "#     s = pd.Series(scores/n_splits/n_repeats,index=X.columns).sort_values(ascending=False)\n",
    "#     s.plot(kind='barh', ax=axes, color=['red' if c.startswith('NOISE') else 'green' for c in s.index])\n",
    "#     axes.invert_yaxis()\n",
    "#     axes.set_xscale('function', functions=(lambda x: np.sign(x)*np.abs(x)**(1/4), lambda x: np.sign(x)*np.abs(x)**4))\n",
    "#     axes.set_title(f'{m} - CROSS-VALIDATED PERMUTATION IMPORTANCE')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "# plt.clf()\n",
    "# plt.close() \n",
    "# plt.rcdefaults()\n",
    "# sns.reset_defaults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if UseFE:\n",
    "    df=df.drop(['clean_title','AT','MT','is_electric','MPFI'],axis=1)\n",
    "    test=test.drop(['clean_title','AT','MT','is_electric','MPFI'],axis=1)\n",
    "else :\n",
    "    df=df.drop(['clean_title'],axis=1)\n",
    "    test=test.drop(['clean_title'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat_params={'boosting_type': 'Ordered',\n",
    "#             'eta': 0.003599532031747313,\n",
    "#             'n_estimators': 785,\n",
    "#             'bootstrap_type': 'Bernoulli',\n",
    "#             'reg_lambda': 0.15814818843258527, 'depth': 5,\n",
    "#             'max_bin': 251,\n",
    "#             'subsample': 0.5767785369941868\n",
    "#            }\n",
    "\n",
    "lgb_params={'n_estimators': 1890,\n",
    "                 'learning_rate': 0.0049343166168420195,\n",
    "                 'data_sample_strategy': 'goss',\n",
    "                 'feature_fraction': 0.3887459059437565, \n",
    "                 'lambda_l1': 7.239967197949322e-07, \n",
    "                 'lambda_l2': 7.488955354504223e-06, \n",
    "                 'num_leaves': 1440, \n",
    "                 'max_depth': 8, \n",
    "                 'colsample_bytree': 0.8390384224124089, \n",
    "                 'min_child_samples': 123, \n",
    "                 'min_gain_to_split': 1.491437722787296, \n",
    "                 'max_bin': 246\n",
    "                }\n",
    "\n",
    "hgb_params={'learning_rate': 0.01896652440172408,\n",
    "            'max_iter': 2291,\n",
    "            'max_leaf_nodes': 1915,\n",
    "            'max_depth': 4, \n",
    "            'min_samples_leaf': 93, \n",
    "            'l2_regularization': 5.726868918336535,\n",
    "            'max_bins': 60, \n",
    "            'tol': 3.5296168509745593e-08\n",
    "           }\n",
    "\n",
    "gb_params={'learning_rate': 0.0359605832217767,\n",
    "           'n_estimators': 626, \n",
    "           'subsample': 0.8182203880912126,\n",
    "           'criterion': 'friedman_mse', \n",
    "           'min_samples_split': 98, \n",
    "           'min_samples_leaf': 94, \n",
    "           'min_weight_fraction_leaf': 0.0039333091548447805, \n",
    "           'max_depth': 13, \n",
    "           'min_impurity_decrease': 0.01946397382407441, \n",
    "           'max_features': 'log2', \n",
    "           'alpha': 0.3688013161101175, \n",
    "           'max_leaf_nodes': 55, \n",
    "           'tol': 9.67892273042649e-05\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=LGBMRegressor(**{**base_params_lgb,**lgb_params})\n",
    "\n",
    "model_pipe=TransformedTargetRegressor(\n",
    "        regressor=model,\n",
    "        transformer=StandardScaler()\n",
    "    )\n",
    "pipe=BaggingRegressor(estimator=model_pipe,n_estimators=200)\n",
    "\n",
    "pipe.fit(X,y)\n",
    "preds=pipe.predict(test)\n",
    "submission[target]=preds\n",
    "submission.to_csv(\"Tunned_lgb_bag200.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=GradientBoostingRegressor(**gb_params, random_state=seed)\n",
    "\n",
    "model_pipe=TransformedTargetRegressor(\n",
    "        regressor=model,\n",
    "        transformer=StandardScaler()\n",
    "    )\n",
    "pipe=BaggingRegressor(estimator=model_pipe,n_estimators=200)\n",
    "\n",
    "pipe.fit(X,y)\n",
    "preds=pipe.predict(test)\n",
    "submission[target]=preds\n",
    "submission.to_csv(\"Tunned_gb_bag200.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=HistGradientBoostingRegressor(**hgb_params,random_state=seed)\n",
    "\n",
    "model_pipe=TransformedTargetRegressor(\n",
    "        regressor=model,\n",
    "        transformer=StandardScaler()\n",
    "    )\n",
    "pipe=BaggingRegressor(estimator=model_pipe,n_estimators=200)\n",
    "\n",
    "pipe.fit(X,y)\n",
    "preds=pipe.predict(test)\n",
    "submission[target]=preds\n",
    "submission.to_csv(\"Tunned_hgb_bag200.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
