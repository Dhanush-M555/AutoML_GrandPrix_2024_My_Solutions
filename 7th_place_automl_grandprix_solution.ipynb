{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import *\n",
    "from lightgbm import *\n",
    "from catboost import *\n",
    "from sklearn.pipeline import *\n",
    "from sklearn.preprocessing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=0\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "train=pd.read_csv(\"/kaggle/input/playground-series-s4e7/train.csv\").drop(\"id\",axis=1)\n",
    "print(\"Train Dataset =\",train.shape)\n",
    "display(train)\n",
    "test=pd.read_csv(\"/kaggle/input/playground-series-s4e7/test.csv\").drop(\"id\",axis=1)\n",
    "print(\"Test Dataset =\",test.shape)\n",
    "display(test)\n",
    "submission=pd.read_csv(\"/kaggle/input/playground-series-s4e7/sample_submission.csv\")\n",
    "\n",
    "origin=pd.read_csv(\"/kaggle/input/health-insurance-cross-sell-prediction-data/train.csv\").drop(\"id\",axis=1)\n",
    "print(\"Original Dataset =\",origin.shape)\n",
    "display(origin)\n",
    "\n",
    "# Target\n",
    "target=\"Response\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_and_convert(df):\n",
    "    df['Gender'] = df['Gender'].map({'Male': 1, 'Female': 0}).astype(int)\n",
    "    df['Vehicle_Age'] = df['Vehicle_Age'].map({'< 1 Year': 1, '1-2 Year': 0, '> 2 Years': 3}).astype(int)\n",
    "    df['Vehicle_Damage'] = df['Vehicle_Damage'].map({'Yes': 1, 'No': 0}).astype(int)\n",
    "\n",
    "# Apply the function to each DataFrame\n",
    "map_and_convert(df)\n",
    "map_and_convert(origin)\n",
    "map_and_convert(train)\n",
    "map_and_convert(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('object')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df\n",
    "\n",
    "df=reduce_mem_usage(df)\n",
    "test=reduce_mem_usage(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_feats=['Gender', \n",
    "           'Age', \n",
    "           'Driving_License', \n",
    "           'Region_Code', \n",
    "           'Previously_Insured',\n",
    "           'Vehicle_Age', \n",
    "           'Vehicle_Damage',\n",
    "        #    'Policy_Sales_Channel',\n",
    "        #    'Vintage'\n",
    "          ]\n",
    "\n",
    "binary_feats=['Gender', \n",
    "               'Driving_License',\n",
    "               'Previously_Insured',\n",
    "               'Vehicle_Damage',\n",
    "          ]\n",
    "\n",
    "num_feats=['Annual_Premium']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering.\n",
    "\n",
    "##### Please Note, that I did not use these Features since they kind of increase performance by a little for untuned model but was not incremental for tuned model and also it becomes computationally expense.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureEngineering(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        x = X.copy()\n",
    "\n",
    "        columns = [\n",
    "                  'Gender', \n",
    "                   'Age', \n",
    "                   'Driving_License', \n",
    "                   'Region_Code', \n",
    "                   'Previously_Insured',\n",
    "                   'Vehicle_Age', \n",
    "                   'Vehicle_Damage', \n",
    "                   'Policy_Sales_Channel', \n",
    "                   'Vintage']\n",
    "        prod_cols=['Region_Code', \n",
    "                   'Policy_Sales_Channel', \n",
    "                   'Vintage']\n",
    "        x['sum_feature'] = x[columns].sum(axis=1)\n",
    "        x['mean_feature'] = x[columns].mean(axis=1)\n",
    "        x['product_feature'] = x[prod_cols].prod(axis=1)\n",
    "        x['max_feature'] = x[columns].max(axis=1)\n",
    "        x['min_feature'] = (x[columns].min(axis=1)).astype(int)\n",
    "        x['std_feature'] = x[columns].std(axis=1)\n",
    "        x['range_feature'] = x[columns].max(axis=1) - x[columns].min(axis=1)\n",
    "        x['variance_feature'] = x[columns].var(axis=1)\n",
    "        x['median'] = x[columns].median(axis=1)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.fit(X, y).transform(X)\n",
    "\n",
    "fe = FeatureEngineering()\n",
    "# df = fe.fit_transform(df)\n",
    "# test=fe.fit_transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### These hyperparameters are `weakly tuned` since it was obtained with few trials and was trained on sample of the training data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter obtained from optuna.\n",
    "\n",
    "\n",
    "lgb_params={'n_estimators': 1530, \n",
    "             'learning_rate': 0.022088462397380975,\n",
    "             'data_sample_strategy': 'bagging',\n",
    "             'feature_fraction': 0.28489795058541373,\n",
    "             'tree_learner': 'feature',\n",
    "             'lambda_l1': 1.383854467743534e-07,\n",
    "             'lambda_l2': 7.277288836793231e-08,\n",
    "             'num_leaves': 1286, 'max_depth': 10, \n",
    "             'subsample_for_bin': 263000,\n",
    "             'colsample_bytree': 0.24600818034167943,\n",
    "             'min_child_samples': 28,\n",
    "             'min_sum_hessian_in_leaf': 4.449615892786049,\n",
    "             'min_gain_to_split': 0.06144443590956064,\n",
    "             'max_bin': 246,\n",
    "             'scale_pos_weight': 2.2558870848705546,\n",
    "             'bagging_freq': 12, 'bagging_fraction': 0.5980677585954857\n",
    "            }\n",
    "\n",
    "cat_params={'boosting_type': 'Plain',\n",
    "             'eta': 0.014670178421179212,\n",
    "             'n_estimators': 2070, 'bootstrap_type': 'Bernoulli',\n",
    "             'reg_lambda': 8.31073825275903, 'depth': 13,\n",
    "             'max_bin': 434, \n",
    "             'scale_pos_weight': 1.2248137884054016,\n",
    "             'grow_policy': 'Depthwise',\n",
    "             'subsample': 0.7290169843199563,\n",
    "             'min_child_samples': 176\n",
    "            }\n",
    "\n",
    "xgb_params={'lambda': 0.015986308208690816,\n",
    "             'alpha': 0.0917043179342634,\n",
    "             'colsample_bytree': 0.9875639808775334,\n",
    "             'subsample': 0.7111941924203469,\n",
    "             'learning_rate': 0.02118413819478032,\n",
    "             'n_estimators': 2230, 'grow_policy': 'depthwise',\n",
    "             'max_depth': 12, 'sampling_method': 'uniform',\n",
    "             'gamma': 0.06573385352184628,\n",
    "             'max_bins': 587,\n",
    "             'min_child_weight': 132,\n",
    "             'max_leaves': 799,\n",
    "             'max_delta_step': 9.507698988457662,\n",
    "             'scale_pos_weight': 3.598883262584229\n",
    "            }\n",
    "\n",
    "xgb=make_pipeline(MEstimateEncoder(cols=cat_feats),\n",
    "                    XGBClassifier(\n",
    "                    objective='binary:logistic', \n",
    "                    eval_metric='auc',\n",
    "                    use_label_encoder=False,\n",
    "                    random_state=42,\n",
    "                    tree_method='gpu_hist',**xgb_params)\n",
    "                        )\n",
    "\n",
    "lgb=make_pipeline(MEstimateEncoder(cols=cat_feats),\n",
    "                   LGBMClassifier(\n",
    "                    metric='auc',\n",
    "                    n_jobs=4,\n",
    "                    verbose=-1,\n",
    "                    random_state=42,\n",
    "                       **lgb_params,\n",
    "                        )\n",
    "                        )\n",
    "\n",
    "cat=CatBoostClassifier(eval_metric='AUC', \n",
    "    task_type=\"GPU\",\n",
    "    silent=True,\n",
    "    random_seed=42,**cat_params )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df.drop(target,axis=1)\n",
    "y=df[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof=pd.DataFrame()\n",
    "test_preds=pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_preds=cross_val_predict(cat,X,y,cv=3,method='predict_proba')\n",
    "oof['catboost']=cat_preds[:,1]\n",
    "display(oof)\n",
    "cat.fit(X,y)\n",
    "test_preds['catboost']=cat.predict_proba(test)[:,1]\n",
    "display(test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_preds=cross_val_predict(xgb,X,y,cv=3,method='predict_proba')\n",
    "oof['xgboost']=xgb_preds[:,1]\n",
    "display(oof)\n",
    "xgb.fit(X,y)\n",
    "test_preds['xgboost']=xgb.predict_proba(test)[:,1]\n",
    "display(test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_preds=cross_val_predict(lgb,X,y,cv=3,method='predict_proba')\n",
    "oof['lightgbm']=lgb_preds[:,1]\n",
    "display(oof)\n",
    "lgb.fit(X,y)\n",
    "test_preds['lightgbm']=lgb.predict_proba(test)[:,1]\n",
    "display(test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "\n",
    "model_ridge_cv = RidgeClassifierCV(alphas=[0.1, 0.5, 0.75, 1, 5, 10, 15], scoring='roc_auc', cv=3)\n",
    "model_ridge_cv.fit(oof, y)\n",
    "coefficients = model_ridge_cv.coef_.flatten()\n",
    "feature_names = oof.columns if isinstance(oof, pd.DataFrame) else [f'feature_{i}' for i in range(oof.shape[1])]\n",
    "\n",
    "coef_df = pd.DataFrame({'Feature': feature_names, 'Coefficient': coefficients})\n",
    "print(\"Coefficients:\")\n",
    "print(coef_df)\n",
    "\n",
    "if hasattr(model_ridge_cv, 'cv_results_'):\n",
    "    cv_results = model_ridge_cv.cv_results_\n",
    "    cv_results_df = pd.DataFrame(cv_results[:, 0, :], columns=[f'alpha_{alpha}' for alpha in model_ridge_cv.alphas])\n",
    "    print(\"Cross-validation results:\")\n",
    "    print(cv_results_df)\n",
    "else:\n",
    "    print(\"Cross-validation results are not available. Ensure store_cv_values=True was set during model initialization.\")\n",
    "\n",
    "intercept = model_ridge_cv.intercept_\n",
    "alpha = model_ridge_cv.alpha_\n",
    "best_score = model_ridge_cv.best_score_\n",
    "\n",
    "print(f\"Intercept: {intercept}\")\n",
    "print(f\"Estimated regularization parameter (alpha): {alpha}\")\n",
    "print(f\"Best score: {best_score}\")\n",
    "\n",
    "#Best Weights were obtained by Ridge Classifier CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {\n",
    "    'catboost': 2.386079,\n",
    "    'xgboost': 2.406132,\n",
    "    'lightgbm': 3.386077\n",
    "}\n",
    "\n",
    "weights_sum = sum(weights.values())\n",
    "weighted_preds = test_preds.apply(lambda col: col * weights[col.name])\n",
    "submission[target] = weighted_preds.sum(axis=1) / weights_sum\n",
    "display(submission)\n",
    "\n",
    "submission.to_csv('ridge_weighted_ensemble_xlc.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightAutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -U lightautoml\n",
    "\n",
    "from lightautoml.automl.presets.tabular_presets import TabularAutoML, TabularUtilizedAutoML\n",
    "from lightautoml.tasks import Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = Task('binary', metric = 'auc')\n",
    "\n",
    "\n",
    "automl = TabularAutoML(\n",
    "    task = task, \n",
    "    timeout = 10*60*60,\n",
    "    cpu_limit = 4,\n",
    "    general_params = {'use_algos': [['linear_l2', 'lgb', 'lgb_tuned', 'cb', 'cb_tuned']]},\n",
    "    reader_params = {'n_jobs': 4}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roles = {'target': target}\n",
    "preds_tr = automl.fit_predict(df, roles = roles, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = automl.predict(test).data[:,  0]\n",
    "lightautoml=submission.copy()\n",
    "lightautoml[target]=preds\n",
    "lightautoml.to_csv('lightautoml_10hr.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Weighted Average of above predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_submission=submission.copy()\n",
    "\n",
    "final_submission[target]=0.4*submission[target]+0.6*lightautoml[target]\n",
    "\n",
    "final_submission.to_csv(\"Ensemble_final.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
